Iteration 1, Best Reward Mean: -202.40, Best Policy Reward: -67.47, Checkpoint Path: /var/folders/zp/9rq_grx133ngp6kcbgsbjbyh0000gn/T/tmpqf4hzhk3
Iteration 3, Best Reward Mean: -199.86, Best Policy Reward: -66.62, Checkpoint Path: /var/folders/zp/9rq_grx133ngp6kcbgsbjbyh0000gn/T/tmpzwwiqglp
Iteration 4, Best Reward Mean: -196.92, Best Policy Reward: -65.64, Checkpoint Path: /var/folders/zp/9rq_grx133ngp6kcbgsbjbyh0000gn/T/tmp26ghfx_9
Iteration 5, Best Reward Mean: -196.00, Best Policy Reward: -65.33, Checkpoint Path: /var/folders/zp/9rq_grx133ngp6kcbgsbjbyh0000gn/T/tmpv5e2k8bk
Iteration 8, Best Reward Mean: -195.82, Best Policy Reward: -65.27, Checkpoint Path: /var/folders/zp/9rq_grx133ngp6kcbgsbjbyh0000gn/T/tmper5kd2th
Iteration 10, Best Reward Mean: -195.50, Best Policy Reward: -65.17, Checkpoint Path: /var/folders/zp/9rq_grx133ngp6kcbgsbjbyh0000gn/T/tmpqpshtb7h
Iteration 17, Best Reward Mean: -194.19, Best Policy Reward: -64.73, Checkpoint Path: /var/folders/zp/9rq_grx133ngp6kcbgsbjbyh0000gn/T/tmpj8qpuesv
Iteration 20, Best Reward Mean: -190.56, Best Policy Reward: -63.52, Checkpoint Path: /var/folders/zp/9rq_grx133ngp6kcbgsbjbyh0000gn/T/tmpzpznc5mv
Iteration 32, Best Reward Mean: -186.77, Best Policy Reward: -62.26, Checkpoint Path: /var/folders/zp/9rq_grx133ngp6kcbgsbjbyh0000gn/T/tmp52cfqlzw
Iteration 58, Best Reward Mean: -185.31, Best Policy Reward: -61.77, Checkpoint Path: /var/folders/zp/9rq_grx133ngp6kcbgsbjbyh0000gn/T/tmp3r9dncox
Iteration 86, Best Reward Mean: -184.50, Best Policy Reward: -61.50, Checkpoint Path: /var/folders/zp/9rq_grx133ngp6kcbgsbjbyh0000gn/T/tmpk9tlypwv
Iteration 92, Best Reward Mean: -184.50, Best Policy Reward: -61.50, Checkpoint Path: /var/folders/zp/9rq_grx133ngp6kcbgsbjbyh0000gn/T/tmp0ndprdzr
Iteration 97, Best Reward Mean: -182.21, Best Policy Reward: -60.74, Checkpoint Path: /var/folders/zp/9rq_grx133ngp6kcbgsbjbyh0000gn/T/tmpwge9nuvo
Iteration 133, Best Reward Mean: -181.56, Best Policy Reward: -60.52, Checkpoint Path: /var/folders/zp/9rq_grx133ngp6kcbgsbjbyh0000gn/T/tmp_m17rmu6
Iteration 172, Best Reward Mean: -174.68, Best Policy Reward: -58.23, Checkpoint Path: /var/folders/zp/9rq_grx133ngp6kcbgsbjbyh0000gn/T/tmp929w9hew

Reference:
policies = {
        "shared_policy": (
            None,  # Use default policy class (PPO)
            temp_env.observation_space,  # Updated observation space
            temp_env.action_space,  # Action space remains the same
            {
                "model": {
                    "fcnet_hiddens": [256, 256, 256],  # Network architecture
                    "fcnet_activation": "relu",
                },
                "framework": "torch",
            },
        )
    }

    # Policy mapping function: All agents use the shared policy
    def policy_mapping_fn(agent_id, episode, worker, **kwargs):
        return "shared_policy"

    # Configure RLlib trainer
    config = (
        PPOConfig()
        .environment(env="no_thanks_env")
        .framework("torch")
        .rollouts(
            num_rollout_workers=6,
            rollout_fragment_length="auto",
        )
        .training(
            train_batch_size=8192,
            sgd_minibatch_size=1024,
            num_sgd_iter=40,  # Increased from 10 to encourage more thorough learning
            lr=1e-3, #5e-5 # Increased from 5e-5 to encourage policy updates
            lr_schedule=[
                [0, 1e-3],     # Start with 1e-3
                [10000, 5e-4], # Decay to 5e-4 at iteration 10000
                [20000, 1e-4], # Decay to 1e-4 at iteration 20000
            ],
            clip_param=0.2,
            entropy_coeff=0.01,  # Increased from 0.001 to encourage exploration
            entropy_coeff_schedule=[
                [0, 0.01],      
                [5000, 0.007],  
                [10000, 0.005], 
                [20000, 0.001], # Decay to exploit more later
            ],
            vf_clip_param=10.0,
            vf_loss_coeff=1.0,
            use_gae=True, # Generalized Advantage Estimation reduces variance in advantage estimates
            lambda_=0.95, # gea_lambda
            #grad_clip=0.5,
            # normalize_rewards=True,
        )
        .multi_agent(
            policies=policies,
            policy_mapping_fn=policy_mapping_fn,
        )
        .resources(
            num_gpus=0
        )
    )